{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r\"c:\\Users\\britt\\Desktop\\YH\\Applicerad AI\\job_discrimination_sandbox\")\n",
    "import re\n",
    "# import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    return get_wordnet_pos(tag)\n",
    "\n",
    "def preprocess_document(doc):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    remove_https = re.sub(r\"http\\S+\", \"\", doc)\n",
    "    remove_com = re.sub(r\"\\ [A-Za-a]*\\.com\", \" \", remove_https)\n",
    "    remove_numbers_punctuations = re.sub(r\"[^a-zA-Z]+\", \" \", remove_com)\n",
    "    pattern = re.compile(r\"\\s+\")\n",
    "    remove_extra_whitespaces = re.sub(pattern, \" \", remove_numbers_punctuations)\n",
    "    only_ascii = unidecode(remove_extra_whitespaces)\n",
    "    doc = only_ascii.lower()\n",
    "\n",
    "    list_of_tokens = word_tokenize(doc)\n",
    "    list_of_tokens_pos = pos_tag(list_of_tokens)\n",
    "    list_of_tokens_wn_pos = [(token[0], penn_to_wn(token[1])) for token in list_of_tokens_pos if token[0] not in stopwords.words(\"english\")]\n",
    "    list_of_lemmas = [lemmatizer.lemmatize(token[0], token[1]) if token[1] != \"\" else lemmatizer.lemmatize(token[0]) for token in list_of_tokens_wn_pos]\n",
    "    cleaned_text = [\" \".join(list_of_lemmas)]\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def get_n_most_important_words_clf(weights, vocabulary, n):\n",
    "    indices = np.argpartition(weights, len(weights) - n)[-n:]\n",
    "    min_elements = weights[indices]\n",
    "    min_elements_order = np.argsort(min_elements)\n",
    "    ordered_indices = indices[min_elements_order]\n",
    "    words = [vocabulary[i] for i in ordered_indices]\n",
    "    weights = [round(weights[i], 5) for i in ordered_indices]\n",
    "\n",
    "    return words[::-1], weights[::-1]\n",
    "\n",
    "def get_25_most_important_words_single_text(text, label, clf_vocabulary, clf_weights):\n",
    "    vocabulary_text = list(set(text[0].split()))\n",
    "    text_weights = []\n",
    "    words, weights = get_n_most_important_words_clf(clf_weights[label], clf_vocabulary, len(clf_vocabulary))\n",
    "    weights_dict = dict(zip(words, weights))\n",
    "    for word in vocabulary_text:\n",
    "        try:\n",
    "            text_weights.append((word, weights_dict[word]))\n",
    "        except KeyError:\n",
    "            pass\n",
    "    \n",
    "    text_weights.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return text_weights[:25]\n",
    "\n",
    "def print_predict_data(file_name, pred_probas, predicted_label, prediction, weights):\n",
    "    labels = [0, 1, 2]\n",
    "    label_to_word = {1: \"female\", 2: \"male\"}\n",
    "\n",
    "    print(f\"The job advertisement {file_name} will most likely (with {round(pred_probas[0][predicted_label] * 100, 2)}% probability) get\", end=\" \")\n",
    "    if 0 < prediction[0]:\n",
    "        print(f\"more than 70% {label_to_word[predicted_label]} applicants.\")\n",
    "    else:\n",
    "        print(\"around as many female as male applicants\")\n",
    "    print(\"The 25 most important words in the advertisement for predicting the above was:\")\n",
    "    for word, weight in weights:\n",
    "        print(f\"{word}: {weight}\")\n",
    "    for label in labels:\n",
    "        if label == predicted_label:\n",
    "            pass\n",
    "        elif label == 0:\n",
    "            print(f\"The probability for the advertisement getting around as many female as male applicants is {round(pred_probas[0][label] * 100, 2)}%\")\n",
    "        else:\n",
    "            if label == 1:\n",
    "                sex = \"female\"\n",
    "            elif label == 2:\n",
    "                sex = \"male\"\n",
    "            print(f\"The probability for the advertisement getting more than 70% {sex} applicants is {round(pred_probas[0][label] * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/models/log_reg_tfidf.pkl\", \"rb\") as read_file:\n",
    "    clf = pickle.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/vectorizers/tfidf.pkl\", \"rb\") as read_file:\n",
    "    vectorizer = pickle.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The job advertisement ASBESTOS WORKER 3435 100518.txt will most likely (with 66.67% probability) get more than 70% male applicants.\n",
      "The 25 most important words in the advertisement for predicting the above was:\n",
      "equipment: 1.20576\n",
      "safety: 0.50725\n",
      "certificate: 0.47912\n",
      "tool: 0.43513\n",
      "material: 0.42955\n",
      "california: 0.34695\n",
      "attach: 0.34616\n",
      "flat: 0.34122\n",
      "rat: 0.33921\n",
      "work: 0.3323\n",
      "license: 0.28845\n",
      "use: 0.28407\n",
      "read: 0.26467\n",
      "copy: 0.26212\n",
      "prior: 0.26204\n",
      "require: 0.26148\n",
      "apprenticeship: 0.26089\n",
      "time: 0.25197\n",
      "cal: 0.25046\n",
      "valid: 0.23466\n",
      "hazardous: 0.23416\n",
      "instruction: 0.22242\n",
      "regulation: 0.21052\n",
      "osha: 0.20628\n",
      "safely: 0.20119\n",
      "The probability for the advertisement getting around as many female as male applicants is 20.45%\n",
      "The probability for the advertisement getting more than 70% female applicants is 12.89%\n"
     ]
    }
   ],
   "source": [
    "clf_weights = clf.coef_\n",
    "clf_vocabulary = vectorizer.get_feature_names_out()\n",
    "file_name = \"ASBESTOS WORKER 3435 100518.txt\"\n",
    "\n",
    "with open(f\"data/original_data/job_bulletins/{file_name}\", \"r\", encoding=\"utf-8\") as f:\n",
    "    application = f.read()\n",
    "\n",
    "text = preprocess_document(application)\n",
    "vectorized_text = vectorizer.transform(text)\n",
    "prediction = clf.predict(vectorized_text)\n",
    "pred_probas = clf.predict_proba(vectorized_text)\n",
    "\n",
    "predicted_label = prediction[0]\n",
    "neutr_prob, female_prob, male_prob = pred_probas[0]\n",
    "weights = get_25_most_important_words_single_text(text, predicted_label, clf_vocabulary, clf_weights)\n",
    "\n",
    "print_predict_data(file_name, pred_probas, predicted_label, prediction, weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "443c96f07534edf61d2e520f7701ba5fe2602e8b5ceabdcfb66f122d33fe2a22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
